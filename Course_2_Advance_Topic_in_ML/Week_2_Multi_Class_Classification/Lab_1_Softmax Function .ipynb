{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee28bd94",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Softmax Function </font>\n",
    "In this lab, we will explore the softmax function. This function is used in both Softmax Regression and in Neural Networks when solving Multiclass Classification problems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a95776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('./Materials_By_Deeplearning/deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib widget\n",
    "from matplotlib.widgets import Slider\n",
    "\n",
    "import sys\n",
    "sys.path.append('./Materials_By_Deeplearning')\n",
    "from lab_utils_common import dlc\n",
    "from lab_utils_softmax import plt_softmax\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8cf33",
   "metadata": {},
   "source": [
    "## Softmax Function\n",
    "In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities.\n",
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n",
    "The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix} \\tag{2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0444f3",
   "metadata": {},
   "source": [
    "Which shows the output is a vector of probabilities. The first entry is the probability the input is the first category given the input $\\mathbf{x}$ and parameters $\\mathbf{w}$ and $\\mathbf{b}$.  \n",
    "Let's create a NumPy implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b09289",
   "metadata": {},
   "source": [
    "## Cost\n",
    "\n",
    "The loss function associated with Softmax, the cross-entropy loss, is:\n",
    "\\begin{equation}\n",
    "  L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Where y is the target category for this example and $\\mathbf{a}$ is the output of a softmax function. In particular, the values in $\\mathbf{a}$ are probabilities that sum to one.\n",
    ">**Recall:** In this course, Loss is for one example while Cost covers all examples. \n",
    " \n",
    " \n",
    "Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. \n",
    "    $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "    1, & \\text{if $y==n$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "Now the cost is:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb0579",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "This lab will discuss two ways of implementing the softmax, cross-entropy loss in Tensorflow, the 'obvious' method and the 'preferred' method. The former is the most straightforward while the latter is more numerically stable.\n",
    "\n",
    "Let's start by creating a dataset to train a multiclass classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc8a5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (2000, 2)\n",
      "y_train (2000,)\n"
     ]
    }
   ],
   "source": [
    "# make  dataset for example\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "x_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)\n",
    "print(\"x_train\",x_train.shape)\n",
    "print(\"y_train\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d939e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(units = 25, activation = \"relu\", name= \"Layer_1\"),\n",
    "        Dense(units = 15, activation = \"relu\", name=\"Layer_2\"),\n",
    "        Dense(units = 4,  activation = \"softmax\", name=\"Layer_3\")\n",
    "    ],name=\"SoftmaxModel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f85caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76494076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 0.9921\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3992\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1687\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1018\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0775\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0652\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0583\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0534\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0506\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x11a6aab4410>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f48ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SoftmaxModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Layer_1 (Dense)             (None, 25)                75        \n",
      "                                                                 \n",
      " Layer_2 (Dense)             (None, 15)                390       \n",
      "                                                                 \n",
      " Layer_3 (Dense)             (None, 4)                 64        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 529 (2.07 KB)\n",
      "Trainable params: 529 (2.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae386ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step\n",
      "[[9.32e-03 2.94e-03 9.70e-01 1.75e-02]\n",
      " [9.91e-01 8.50e-03 9.11e-05 1.28e-06]\n",
      " [9.58e-01 4.12e-02 7.80e-04 3.44e-05]\n",
      " ...\n",
      " [3.36e-03 9.90e-01 2.76e-04 6.79e-03]\n",
      " [3.39e-05 3.10e-04 1.90e-04 9.99e-01]\n",
      " [4.80e-03 8.80e-04 9.92e-01 2.48e-03]]\n",
      "largest value 0.99999654 smallest value 1.4348449e-10\n"
     ]
    }
   ],
   "source": [
    "Model_Pridect = model.predict(x_train)\n",
    "print(Model_Pridect)\n",
    "print(\"largest value\", np.max(Model_Pridect), \"smallest value\", np.min(Model_Pridect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3174c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01 0.   0.97 0.02], category: 2\n",
      "[9.91e-01 8.50e-03 9.11e-05 1.28e-06], category: 0\n",
      "[9.58e-01 4.12e-02 7.80e-04 3.44e-05], category: 0\n",
      "[6.43e-03 9.89e-01 2.89e-04 4.59e-03], category: 1\n",
      "[2.91e-03 3.64e-05 9.97e-01 7.38e-05], category: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print( f\"{Model_Pridect[i]}, category: {np.argmax(Model_Pridect[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf79f87",
   "metadata": {},
   "source": [
    "### <b> As wel learn that we our model is making round off error because computer is having very limited space to store number in a float formate so to reduce rounfoff error  we're going to reduce the user of variable and directly provide the computation into the loss function so that the error becomes as much as less possible </b> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d17677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this now we are modifing the model\n",
    "model_Reduce_Error = Sequential(\n",
    "    \n",
    "    [\n",
    "        Dense(units = 25, activation=\"relu\", name=\"Layer_1\"),\n",
    "        Dense(units = 15, activation=\"relu\", name=\"Layer_2\"),\n",
    "        Dense(units = 4, activation=\"linear\", name=\"Layer_3\")\n",
    "        \n",
    "    ], name = \"SOFTMAX_ROUNDOFF_ERROR_SOLUTION\"\n",
    ")\n",
    "\n",
    "model_Reduce_Error.compile(\n",
    "\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer = tf.keras.optimizers.Adam(0.001)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1a19dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 0.9006\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3836\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1862\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1071\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0745\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0587\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0499\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0437\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0394\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x11a6be05110>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Reduce_Error.fit(x_train,y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdae0378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SOFTMAX_ROUNDOFF_ERROR_SOLUTION\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Layer_1 (Dense)             (None, 25)                75        \n",
      "                                                                 \n",
      " Layer_2 (Dense)             (None, 15)                390       \n",
      "                                                                 \n",
      " Layer_3 (Dense)             (None, 4)                 64        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 529 (2.07 KB)\n",
      "Trainable params: 529 (2.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\\n\",model_Reduce_Error.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6bdbba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step\n",
      "two example output vectors:\n",
      " [[-2.03  0.34  5.17  0.06]\n",
      " [ 6.74  1.74 -1.17 -8.27]\n",
      " [ 5.    1.68 -0.95 -6.55]\n",
      " ...\n",
      " [-3.25  4.2  -0.42 -1.08]\n",
      " [-8.74 -0.17  0.14  8.35]\n",
      " [-0.44  0.49  5.86 -1.6 ]]\n",
      "largest value 15.194508 smallest value -13.431956\n"
     ]
    }
   ],
   "source": [
    "Model_Pri_less_err = model_Reduce_Error.predict(x_train)\n",
    "print(f\"two example output vectors:\\n {Model_Pri_less_err}\")\n",
    "print(\"largest value\", np.max(Model_Pri_less_err), \"smallest value\", np.min(Model_Pri_less_err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
