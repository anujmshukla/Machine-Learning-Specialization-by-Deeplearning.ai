{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa173b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Sucessfully\n"
     ]
    }
   ],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./Deeplearning_Material/deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays\n",
    "print(\"Run Sucessfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d46b0e",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2\"></a>\n",
    "# Problem Statement\n",
    "\n",
    "You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f49a63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Sucessfully\n"
     ]
    }
   ],
   "source": [
    "#giving 3 tranning data\n",
    "X_Train = np.array([[2104,5,1,45],[1416,3,2,40],[852,2,1,35]])\n",
    "Y_Train = np.array([460,232,178])\n",
    "print(\"Run Sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad8e3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train have (3, 4) size and elements are:\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "Y_train have (3,) size and elements are:\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "#printing the data \n",
    "print(f\"X_train have {X_Train.shape} size and elements are:\\n{X_Train}\")\n",
    "print(f\"Y_train have {Y_Train.shape} size and elements are:\\n{Y_Train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be7228",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.2\"></a>\n",
    "## Parameter vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b818c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,),  w_init type: <class 'numpy.ndarray'>\n",
      "b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape},  w_init type: {type(w_init)}\")\n",
    "print(f\"b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d22d63",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "# Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66a82ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Sucessfully\n"
     ]
    }
   ],
   "source": [
    "def Predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    \n",
    "\n",
    "print(\"Run Sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50e95876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched row from tranning dataset:  [2104    5    1   45]\n",
      "predicted value is 459.9999976194083\n",
      "predicted value after rounf off is  460.0000\n"
     ]
    }
   ],
   "source": [
    "#lets get a row from our traning dataset and make Prediction according to the our manually set variable by W Vector and P\n",
    "temp_X = X_Train[0,]\n",
    "print(\"Fetched row from tranning dataset: \",temp_X)\n",
    "temp_f_wb = Predict(temp_X,w_init,b_init)\n",
    "print(f\"predicted value is {temp_f_wb}\")\n",
    "print(f\"predicted value after rounf off is  {temp_f_wb:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919a151",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8f77fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Sucessfully\n"
     ]
    }
   ],
   "source": [
    "def Compute_Cost(x,y,w,b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    \n",
    "    sum_Cost = 0\n",
    "    sizeOfInput = x.shape[0] #return how many rows are there in data\n",
    "    for i in range(0,sizeOfInput):\n",
    "        f_wb = np.dot(x[i],w) + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        sum_Cost += cost\n",
    "    \n",
    "    total_cost = (1/(2*sizeOfInput))*sum_Cost\n",
    "    return total_cost\n",
    "    \n",
    "    \n",
    "print(\"Run Sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54e4ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost is 1.5578904428966628e-12\n"
     ]
    }
   ],
   "source": [
    "#lets Find out cost our manually seted parameters \n",
    "cost = Compute_Cost(X_Train,Y_Train,w_init,b_init)\n",
    "print(\"The cost is\",cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8934fa",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "# 5 Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f5280a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Sucessfully\n"
     ]
    }
   ],
   "source": [
    "def Compute_Gradient(x,y,w,b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    \n",
    "    Num_of_rows, Num_of_columns = x.shape \n",
    "    sum_dj_dw = 0\n",
    "    sum_db_dw = 0\n",
    "    \n",
    "    for i in range(0,Num_of_rows):\n",
    "        f_wb = np.dot(x[i],w) + b\n",
    "        diffrence = f_wb - y[i]\n",
    "        sigmaTermFor_dj_dw = diffrence*x[i]\n",
    "        sum_dj_dw += sigmaTermFor_dj_dw\n",
    "        sum_db_dw += diffrence\n",
    "        \n",
    "    dj_dw = (1/Num_of_rows)*sum_dj_dw\n",
    "    dj_db = (1/Num_of_rows)*sum_db_dw\n",
    "    \n",
    "    return dj_dw,dj_db\n",
    "\n",
    "print(\"Run Sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d489eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Sucessfully\n"
     ]
    }
   ],
   "source": [
    "def Compute_Gradient_Descent(x, y, w_in, b_in, iteration, alpha, cost_function, Gradient_Descent ):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      J_History        : History of cost function changes with iteration \n",
    "    \"\"\"\n",
    "    J_History = []\n",
    "    \n",
    "    for i in range(0,iteration):\n",
    "        \n",
    "        #getting Gradient\n",
    "        dj_dw,dj_db = Gradient_Descent(x,y,w_in,b_in)\n",
    "        \n",
    "        w_in = w_in - alpha*dj_dw  # \"w_in\" is an array so \"- alpha*dj_dw\" operation will perfrom on all elements of an array \n",
    "        b_in = b_in - alpha*dj_db\n",
    "        \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_History.append( cost_function(x, y, w_in, b_in))\n",
    "            \n",
    "        #print cost after each 100 iterarion \n",
    "        if(i%100==0):\n",
    "            print(f\"Iteration {i:4d}: Cost {J_History[-1]:8.2f}   \")\n",
    "            \n",
    "    return w_in, b_in, J_History\n",
    "            \n",
    "print(\"Run Sucessfully\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b4a9294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "the final w is  [ 0.2   0.   -0.01 -0.07]\n",
      "the final b is  -0.002235407530932535\n"
     ]
    }
   ],
   "source": [
    "w_init = np.zeros(4)\n",
    "b_init = 0\n",
    "iteration = 1000\n",
    "alpha = 5.0e-7\n",
    "\n",
    "#running gradient descent \n",
    "w_init, b_init, j_history = Compute_Gradient_Descent(X_Train,Y_Train,w_init,b_init,iteration,alpha,Compute_Cost,Compute_Gradient)\n",
    "\n",
    "print(\"the final w is \",w_init)\n",
    "print(\"the final b is \",b_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae844ff9",
   "metadata": {},
   "source": [
    "### Hear we have sucesfully find the w and b automatically but yet we see that there is sitll high cost is seen which is not favourable\n",
    "\n",
    "let's test model and know how is it performing \n",
    "we will test model by taking tranning data as input data and notice it diffrence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "358a5bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 0th input set predicted value 426.1853 is and actual value is 460.0000 and differences is 33.8147\n",
      "for 1th input set predicted value 286.1675 is and actual value is 232.0000 and differences is -54.1675\n",
      "for 2th input set predicted value 171.4676 is and actual value is 178.0000 and differences is 6.5324\n"
     ]
    }
   ],
   "source": [
    "size,_ = X_Train.shape\n",
    "for i in range(0,size):\n",
    "    prid_value = Predict(X_Train[i],w_init,b_init)\n",
    "    diff = Y_Train[i] - prid_value\n",
    "    print(f\"for {i}th input set predicted value {prid_value:0.4f} is and actual value is {Y_Train[i]:0.4f} and differences is {diff:0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
